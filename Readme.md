# Least Squares, L1 and L2 regularization, Gradient Descent implementation, and Line Search

This project implements various optimization techniques from scratch, including:

- **Least Squares**: A method to minimize the sum of the squares of the differences between observed and predicted values.
- **L2 Regularization**: Also known as Ridge Regression, it adds a penalty equal to the square of the magnitude of coefficients to the loss function to prevent overfitting.
- **L1 Regularization**: Also known as Lasso Regression, it adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function, promoting sparsity.
- **Gradient Descent**: An optimization algorithm used to minimize the loss function by iteratively moving towards the steepest descent direction.
- **Gradient Descent to Minimize**: A specific application of gradient descent to find the minimum value of a function.
- **Line Search**: An optimization technique that finds the optimal step size along a given search direction to minimize the objective function.

### Dependencies

numpy
matplotlib
pandas
